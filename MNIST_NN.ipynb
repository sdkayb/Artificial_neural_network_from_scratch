{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST hand written digits recognition with deep learning \n",
    "---\n",
    "## projet réalisé par : ELARNI Maroua & SADKI Ayoub\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import scipy as sc \n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    " #to normalize input data\n",
    "    def normalize(x):\n",
    "    x = x/255.\n",
    "    return x\n",
    "\n",
    "#k = 21\n",
    "#to subfold data \n",
    "def define_datafolds(k, S , y):\n",
    "    data_set_folds =[]\n",
    "    labels_folds=[]\n",
    "    v= y.shape\n",
    "    for i in range(k):\n",
    "        data_set_folds.append( S[int(y.shape[0]/k)*i : int(y.shape[0]/k)*(i+1),:] )   \n",
    "        labels_folds.append( y[int(y.shape[0]/k)*i : int(y.shape[0]/k)*(i+1)] )   \n",
    "    return np.array(data_set_folds) , np.array(labels_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input , layer n nodes , output\n",
    "#to create the flexible neurone network \n",
    "def make_nn( input_dim = 784 , nLayers = 18 , nNodes = 17 , out_dim= 10 ) :\n",
    "    weights_matrices = []\n",
    "    biais_matrices = []\n",
    "    nHiddenLayers = nLayers - 2\n",
    "    \n",
    "    input_weights = weights_matrices.append(   np.random.rand(nNodes, input_dim) -0.5)\n",
    "    biais_input = biais_matrices.append(   np.random.rand(nNodes, 1) -0.5)\n",
    "\n",
    "#     input_weights = weights_matrices.append(   np.ones((nNodes, input_dim)))\n",
    "#     biais_input = biais_matrices.append(   np.ones((nNodes, 1)))\n",
    "\n",
    "    for i in range(nHiddenLayers-1):\n",
    "        weights_matrices.append(   np.random.rand(nNodes, nNodes)-0.5)\n",
    "        biais_matrices.append(   np.random.rand(nNodes, 1)-0.5)\n",
    "\n",
    "#         weights_matrices.append(np.ones((nNodes, nNodes)))\n",
    "#         biais_matrices.append( np.ones((nNodes, 1)))\n",
    "    \n",
    "    output_weights = weights_matrices.append(   np.random.rand(out_dim, nNodes)-0.5)\n",
    "    biais_output = biais_matrices.append(   np.random.rand(out_dim, 1)-0.5)\n",
    "\n",
    "#     output_weights = weights_matrices.append(   np.ones((out_dim, nNodes)))\n",
    "#     biais_output = biais_matrices.append(   np.ones((out_dim, 1)))\n",
    "    \n",
    "    return weights_matrices , biais_matrices\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def forward_propagation(weights_matrices , biais_matrices, x):\n",
    "    lin_out = []\n",
    "    act_out = []\n",
    "    print(\"input shape :\" , x.shape)\n",
    "    #x = x.reshape((x.shape[0],400))\n",
    "    x = x.T\n",
    "    for i in range(len(weights_matrices)-1):\n",
    "       # print(\"loop: \", i)\n",
    "       # print(\"weights shape\" , np.array(weights_matrices[i]).shape)\n",
    "        #print(\"biais shape\" , np.array(biais_matrices[i]).shape)\n",
    "        x = (np.array(weights_matrices[i]) @ x) + np.array(biais_matrices[i])\n",
    "        lin_out.append(x)\n",
    "        x = Relu(x)\n",
    "        act_out.append(x)\n",
    "        #print(\"New input shape:\", x.shape)\n",
    "    y_pred = softmax(np.array(weights_matrices[-1]) @ x + np.array(biais_matrices[-1]))\n",
    "    return lin_out, act_out ,y_pred\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    one_hot = np.zeros((10 , y.shape[0])).T\n",
    "    for i in range(y.shape[0]):\n",
    "            one_hot[i][y[i]] = 1  \n",
    "    return one_hot.T\n",
    "\n",
    "def backward_propagation(lin_out, act_out, y_pred, weights_matrices, y,  x):\n",
    "    m = x.shape[0]\n",
    "    y_enc = one_hot_encode(y)\n",
    "    #print(\"y_enc\", y_enc.shape)\n",
    "    #print(\"y_pred\", y_pred.shape)\n",
    "    dZ, dW, dB =[None]*len(weights_matrices) , [None]*len(weights_matrices), [None]*len(weights_matrices)\n",
    "    #print(\"dZ=\",dZ.shape)\n",
    "    dZ[0]= - y_enc + y_pred \n",
    "    #print(\"dZ[0]=\",dZ[0].shape)\n",
    "    #print(\"actout[0]=\",act_out[-1].shape)\n",
    "    dW[0] = (1/m)*dZ[0].dot(act_out[-1].T)# dW(4)\n",
    "    dB[0] = (1/m)*np.sum(dZ[0] , axis = 1)\n",
    "    #print(\"dz0:  \" , dZ[0])\n",
    "    #print(\"db0 = \" , dB[0])\n",
    "    for i in range(len(weights_matrices)-1,1,-1):\n",
    "       # print(i)\n",
    "      # print(len(lin_out))\n",
    "        dZ[len(weights_matrices)-i] = weights_matrices[i].T.dot(dZ[len(weights_matrices)-i-1]) * ReLU_deriv(lin_out[i-1])\n",
    "        dW[len(weights_matrices)-i] = (1/m) * dZ[len(weights_matrices)-i].dot(act_out[i-2].T)\n",
    "        dB[len(weights_matrices)-i]  = (1/m) * np.sum(dZ[len(weights_matrices)-i], axis = 1)\n",
    "    dZ[len(weights_matrices)-1] = weights_matrices[1].T.dot(dZ[len(weights_matrices)-2]) * ReLU_deriv(lin_out[0])\n",
    "    dW[len(weights_matrices)-1] = (1/m) * dZ[len(weights_matrices)-1].dot(x)\n",
    "    dB[len(weights_matrices)-1]  =( 1/m) * np.sum(dZ[len(weights_matrices)-1] , axis = 1)\n",
    "    #print(len(dW), len(dB))\n",
    "    return dW , dB\n",
    "#     return np.array(dW) ,np.array(dB)\n",
    "\n",
    "def update_params(weights_matrices, dW, biais_matrices, dB, alpha):\n",
    "    #print(\"dw shape : \" , dW[16].shape)\n",
    "#     print(\"weig mat shape : \" , weights_matrices[0].shape)\n",
    "    dW=dW[::-1]\n",
    "    dB=dB[::-1]\n",
    "#     print(\"dw :\" , dW)\n",
    "#     print(\"we mat : \" ,weights_matrices)\n",
    "    #print(\"db : \", dB)\n",
    "    #print(\"biais : \\n\" , biais_matrices)\n",
    "    for i in range(len(weights_matrices)) :\n",
    "        weights_matrices[i] = weights_matrices[i] - alpha * dW[i]\n",
    "        #print( \"biais before: \" , biais_matrices[i] , \" shape : \" , biais_matrices[i].shape)\n",
    "        #print( \"dB before: \" , dB[i] , \" shape : \" , dB[i].shape)\n",
    "        biais_matrices[i] = biais_matrices[i] - alpha * dB[i].reshape((dB[i].shape[0] , 1))\n",
    "#         print(\"biais after \\n\" , biais_matrices )\n",
    "    return weights_matrices, biais_matrices\n",
    "\n",
    "def gradient_descent(X,Y, alpha, iterations):\n",
    "    weights_matrices , biais_matrices = make_nn( input_dim = 784 , nLayers = 3 , nNodes = 4, out_dim= 10)\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration: \", i)\n",
    "        print(weights_matrices[0])\n",
    "        lin_out, act_out ,y_pred = forward_propagation(weights_matrices , biais_matrices, X)\n",
    "        dW, dB = backward_propagation(lin_out, act_out ,y_pred, weights_matrices, Y, X)\n",
    "        weights_matrices, biais_matrices = update_params(weights_matrices, dW, biais_matrices, dB, alpha)\n",
    "    _,_,y_pred = forward_propagation(weights_matrices , biais_matrices, X)\n",
    "    return weights_matrices, biais_matrices, y_pred \n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return str(round((np.sum(np.argmax(predictions, axis=0) == Y) / Y.shape)[0],2)*100) + '%'\n",
    "\n",
    "def predict(weights_matrices, biais_matrices, Xtest):\n",
    "    _,_,y_test = forward_propagation(weights_matrices , biais_matrices, Xtest)\n",
    "    return y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### les données utilisés ne sont pas celles du tp 4:\n",
    "---\n",
    " on a rencontré trop de problème en les utilisant et pour contourner ce problème on a eu recourt a une autre base de donnée MNIST qu'on a récupérer sur kaggle (train.csv) , on a travaillé avec 10000 images pour le training et 1000 pour le test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data to train and test :\n",
    "X_train = data.to_numpy()[:10000 , 1:]\n",
    "y_train = data.to_numpy()[:10000 , 0]\n",
    "\n",
    "X_test = data.to_numpy()[10000:11000 , 1:]\n",
    "y_test = data.to_numpy()[10000:11000 , 0]\n",
    "\n",
    "#normalisation des données :\n",
    "X = normalize(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_iter = 3000\n",
    "alpha = 0.1\n",
    "weights_matrices, biais_matrices, y_pred = gradient_descent(X , y , alpha, nbr_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_folds , labels_folds = define_datafolds(21, X , y)\n",
    "# X_train = np.delete(data_set_folds, 13 , axis=0)\n",
    "# y_train =  np.delete(labels_folds, 13, axis=0)\n",
    "# X_test = data_set_folds[13]\n",
    "# y_test = labels_folds[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution on test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = normalize(X_test)\n",
    "f = predict(weights_matrices, biais_matrices, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measures on a test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.0%\n",
      "Precision:  84.0 %\n",
      "Recall: 84.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(y_pred , y)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "precision = round(precision_score(y_test,np.argmax(f, axis=0),  average='macro'),2)\n",
    "print(\"Precision: \",str(precision*100),\"%\")\n",
    "recall = round(recall_score(y_test,np.argmax(f, axis=0),  average='macro'),2)\n",
    "print(\"Recall:\",str(recall*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
